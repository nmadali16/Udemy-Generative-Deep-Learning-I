{"cells":[{"cell_type":"code","execution_count":null,"id":"a63c0b96","metadata":{"id":"a63c0b96"},"outputs":[],"source":["!pip install labml_nn"]},{"cell_type":"code","execution_count":null,"id":"c1fcb4d9","metadata":{"id":"c1fcb4d9"},"outputs":[],"source":["import math\n","from pathlib import Path\n","from typing import Iterator, Tuple\n","import numpy as np\n","import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","\n","from torchvision import datasets, transforms, utils\n","\n","from labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n","from labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n","from labml_nn.utils import cycle_dataloader"]},{"cell_type":"code","execution_count":null,"id":"70a80590","metadata":{"id":"70a80590"},"outputs":[],"source":["from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"id":"14e7ded4","metadata":{"id":"14e7ded4"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    ## Dataset\n","\n","    This loads the training dataset and resize it to the give image size.\n","    \"\"\"\n","\n","    def __init__(self, path: str, image_size: int):\n","        \"\"\"\n","        * `path` path to the folder containing the images\n","        * `image_size` size of the image\n","        \"\"\"\n","        super().__init__()\n","\n","        # Get the paths of all `jpg` files\n","        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n","\n","        # Transformation\n","        self.transform = torchvision.transforms.Compose([\n","            # Resize the image\n","            torchvision.transforms.Resize((image_size,image_size)),\n","            # Convert to PyTorch tensor\n","            torchvision.transforms.ToTensor(),\n","        ])\n","\n","    def __len__(self):\n","        \"\"\"Number of images\"\"\"\n","        return len(self.paths)\n","\n","    def __getitem__(self, index):\n","        \"\"\"Get the the `index`-th image\"\"\"\n","        path = self.paths[index]\n","        img = Image.open(path)\n","        return self.transform(img)"]},{"cell_type":"code","execution_count":null,"id":"310cd524","metadata":{"id":"310cd524"},"outputs":[],"source":["device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"e1637f94","metadata":{"id":"e1637f94"},"outputs":[],"source":["# <a id=\"dataset_path\"></a>\n","# We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n","# You can find the download instruction in this\n","# [discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n","# Save the images inside `data/stylegan` folder.\n","dataset_path: str = os.path.join('data_faces','img_align_celeba')\n","\n","# Batch size\n","batch_size: int = 32\n","# Dimensionality of $z$ and $w$\n","d_latent: int = 512\n","# Height/width of the image\n","image_size: int = 64\n","# Number of layers in the mapping network\n","mapping_network_layers: int = 8\n"]},{"cell_type":"code","execution_count":null,"id":"814e55e3","metadata":{"id":"814e55e3"},"outputs":[],"source":["# [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n","gradient_penalty = GradientPenalty()\n","# Gradient penalty coefficient $\\gamma$\n","gradient_penalty_coefficient: float = 10.\n","\n","# [Path length penalty](index.html#path_length_penalty)\n","path_length_penalty: PathLengthPenalty"]},{"cell_type":"code","execution_count":null,"id":"266bc991","metadata":{"id":"266bc991"},"outputs":[],"source":["### Initialize\n","\n","# Create dataset\n","dataset = Dataset(dataset_path,image_size)\n","# Create data loader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, drop_last=True)\n","# Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n","loader = cycle_dataloader(dataloader)"]},{"cell_type":"code","execution_count":null,"id":"c271dfaf","metadata":{"id":"c271dfaf"},"outputs":[],"source":["# $\\log_2$ of image resolution\n","log_resolution = int(math.log2(image_size))\n","\n","# Create discriminator and generator\n","discriminator = Discriminator(log_resolution).to(device)\n","generator = Generator(log_resolution, d_latent).to(device)\n","# Get number of generator blocks for creating style and noise inputs\n","n_gen_blocks = generator.n_blocks\n","# Create mapping network\n","mapping_network = MappingNetwork(d_latent, mapping_network_layers).to(device)\n","# Create path length penalty loss\n","path_length_penalty = PathLengthPenalty(0.99).to(device)\n"]},{"cell_type":"code","execution_count":null,"id":"1eec6099","metadata":{"id":"1eec6099"},"outputs":[],"source":["# Generator & Discriminator learning rate\n","learning_rate: float = 1e-3\n","# Mapping network learning rate ($100 \\times$ lower than the others)\n","mapping_network_learning_rate: float = 1e-5\n","# Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n","gradient_accumulate_steps: int = 1\n","# $\\beta_1$ and $\\beta_2$ for Adam optimizer\n","adam_betas: Tuple[float, float] = (0.0, 0.99)\n","# Probability of mixing styles\n","style_mixing_prob: float = 0.9"]},{"cell_type":"code","execution_count":null,"id":"a776f493","metadata":{"id":"a776f493"},"outputs":[],"source":["# Discriminator and generator losses\n","discriminator_loss = DiscriminatorLoss().to(device)\n","generator_loss = GeneratorLoss().to(device)\n","\n","# Create optimizers\n","discriminator_optimizer = torch.optim.Adam( discriminator.parameters(), lr=learning_rate, betas=adam_betas )\n","generator_optimizer = torch.optim.Adam( generator.parameters(), lr=learning_rate, betas=adam_betas )\n","mapping_network_optimizer = torch.optim.Adam(mapping_network.parameters(), lr=mapping_network_learning_rate, betas=adam_betas )"]},{"cell_type":"code","execution_count":null,"id":"1b26ad7a","metadata":{"id":"1b26ad7a"},"outputs":[],"source":["# The interval at which to compute gradient penalty\n","lazy_gradient_penalty_interval: int = 4\n","# Path length penalty calculation interval\n","lazy_path_penalty_interval: int = 32\n","# Skip calculating path length penalty during the initial phase of training\n","lazy_path_penalty_after: int = 5_000\n","\n","# How often to log generated images\n","log_generated_interval: int = 500\n","# How often to save model checkpoints\n","save_checkpoint_interval: int = 2_000"]},{"cell_type":"code","execution_count":null,"id":"c41281f3","metadata":{"id":"c41281f3"},"outputs":[],"source":["if not os.path.exists(\"checkpoints\"):\n","    os.makedirs(\"checkpoints\")"]},{"cell_type":"code","execution_count":null,"id":"d4c9b580","metadata":{"id":"d4c9b580","outputId":"ebb43800-68ae-47f7-c04a-cb28a37fb3d5"},"outputs":[{"data":{"text/plain":["'   \\ndef step( idx: int):\\n        \"\"\"\\n        ### Training Step\\n        \"\"\"\\n\\n        # Train the discriminator\\n        with monit.section(\\'Discriminator\\'):\\n            # Reset gradients\\n            discriminator_optimizer.zero_grad()\\n\\n            # Accumulate gradients for `gradient_accumulate_steps`\\n            for i in range(gradient_accumulate_steps):\\n               \\n                    # Sample images from generator\\n                    generated_images, _ = generate_images(batch_size)\\n                    # Discriminator classification for generated images\\n                    fake_output = discriminator(generated_images.detach())\\n\\n                    # Get real images from the data loader\\n                    real_images = next(loader).to(device)\\n                    # We need to calculate gradients w.r.t. real images for gradient penalty\\n                    if (idx + 1) % lazy_gradient_penalty_interval == 0:\\n                        real_images.requires_grad_()\\n                    # Discriminator classification for real images\\n                    real_output = discriminator(real_images)\\n\\n                    # Get discriminator loss\\n                    real_loss, fake_loss = discriminator_loss(real_output, fake_output)\\n                    disc_loss = real_loss + fake_loss\\n\\n                    # Add gradient penalty\\n                    if (idx + 1) % lazy_gradient_penalty_interval == 0:\\n                        # Calculate and log gradient penalty\\n                        gp = gradient_penalty(real_images, real_output)\\n                        tracker.add(\\'loss.gp\\', gp)\\n                        # Multiply by coefficient and add gradient penalty\\n                        disc_loss = disc_loss + 0.5 * gradient_penalty_coefficient * gp * lazy_gradient_penalty_interval\\n\\n                    # Compute gradients\\n                    disc_loss.backward()\\n\\n                    # Log discriminator loss\\n                    tracker.add(\\'loss.discriminator\\', disc_loss)\\n\\n            if (idx + 1) % log_generated_interval == 0:\\n                # Log discriminator model parameters occasionally\\n                tracker.add(\\'discriminator\\', discriminator)\\n\\n            # Clip gradients for stabilization\\n            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\\n            # Take optimizer step\\n            discriminator_optimizer.step()\\n\\n        # Train the generator\\n        with monit.section(\\'Generator\\'):\\n            # Reset gradients\\n            generator_optimizer.zero_grad()\\n            mapping_network_optimizer.zero_grad()\\n\\n            # Accumulate gradients for `gradient_accumulate_steps`\\n            for i in range(gradient_accumulate_steps):\\n                # Sample images from generator\\n                generated_images, w = generate_images(batch_size)\\n                # Discriminator classification for generated images\\n                fake_output = discriminator(generated_images)\\n\\n                # Get generator loss\\n                gen_loss = generator_loss(fake_output)\\n\\n                # Add path length penalty\\n                if idx > lazy_path_penalty_after and (idx + 1) % lazy_path_penalty_interval == 0:\\n                    # Calculate path length penalty\\n                    plp = path_length_penalty(w, generated_images)\\n                    # Ignore if `nan`\\n                    if not torch.isnan(plp):\\n                        tracker.add(\\'loss.plp\\', plp)\\n                        gen_loss = gen_loss + plp\\n\\n                # Calculate gradients\\n                gen_loss.backward()\\n\\n                # Log generator loss\\n                tracker.add(\\'loss.generator\\', gen_loss)\\n\\n            if (idx + 1) % log_generated_interval == 0:\\n                # Log discriminator model parameters occasionally\\n                tracker.add(\\'generator\\', generator)\\n                tracker.add(\\'mapping_network\\', mapping_network)\\n\\n            # Clip gradients for stabilization\\n            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\\n            torch.nn.utils.clip_grad_norm_(mapping_network.parameters(), max_norm=1.0)\\n\\n            # Take optimizer step\\n            generator_optimizer.step()\\n            mapping_network_optimizer.step()\\n\\n        # Log generated images\\n        if (idx + 1) % log_generated_interval == 0:\\n            tracker.add(\\'generated\\', torch.cat([generated_images[:6], real_images[:3]], dim=0))\\n        # Save model checkpoints\\n        if (idx + 1) % save_checkpoint_interval == 0:\\n            experiment.save_checkpoint()\\n\\n        # Flush tracker\\n        tracker.save()\\n       \\n\\n'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def get_w( batch_size: int):\n","        \"\"\"\n","        ### Sample $w$\n","\n","        This samples $z$ randomly and get $w$ from the mapping network.\n","\n","        We also apply style mixing sometimes where we generate two latent variables\n","        $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n","        Then we randomly sample a cross-over point and apply $w_1$ to\n","        the generator blocks before the cross-over point and\n","        $w_2$ to the blocks after.\n","        \"\"\"\n","\n","        # Mix styles\n","        if torch.rand(()).item() < style_mixing_prob:\n","            # Random cross-over point\n","            cross_over_point = int(torch.rand(()).item() * n_gen_blocks)\n","            # Sample $z_1$ and $z_2$\n","            z2 = torch.randn(batch_size, d_latent).to(device)\n","            z1 = torch.randn(batch_size, d_latent).to(device)\n","            # Get $w_1$ and $w_2$\n","            w1 = mapping_network(z1)\n","            w2 = mapping_network(z2)\n","            # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n","            w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n","            w2 = w2[None, :, :].expand(n_gen_blocks - cross_over_point, -1, -1)\n","            return torch.cat((w1, w2), dim=0)\n","        # Without mixing\n","        else:\n","            # Sample $z$ and $z$\n","            z = torch.randn(batch_size, d_latent).to(device)\n","            # Get $w$ and $w$\n","            w = mapping_network(z)\n","            # Expand $w$ for the generator blocks\n","            return w[None, :, :].expand(n_gen_blocks, -1, -1)\n","\n","def get_noise( batch_size: int):\n","        \"\"\"\n","        ### Generate noise\n","\n","        This generates noise for each [generator block](index.html#generator_block)\n","        \"\"\"\n","        # List to store noise\n","        noise = []\n","        # Noise resolution starts from $4$\n","        resolution = 4\n","\n","        # Generate noise for each generator block\n","        for i in range(n_gen_blocks):\n","            # The first block has only one $3 \\times 3$ convolution\n","            if i == 0:\n","                n1 = None\n","            # Generate noise to add after the first convolution layer\n","            else:\n","                n1 = torch.randn(batch_size, 1, resolution, resolution, device=device)\n","            # Generate noise to add after the second convolution layer\n","            n2 = torch.randn(batch_size, 1, resolution, resolution, device=device)\n","\n","            # Add noise tensors to the list\n","            noise.append((n1, n2))\n","\n","            # Next block has $2 \\times$ resolution\n","            resolution *= 2\n","\n","        # Return noise tensors\n","        return noise\n","\n","def generate_images( batch_size: int):\n","        \"\"\"\n","        ### Generate images\n","\n","        This generate images using the generator\n","        \"\"\"\n","\n","        # Get $w$\n","        w = get_w(batch_size)\n","        # Get noise\n","        noise = get_noise(batch_size)\n","\n","        # Generate images\n","        images = generator(w, noise)\n","\n","        # Return images and $w$\n","        return images, w\n"]},{"cell_type":"code","execution_count":null,"id":"64672bef","metadata":{"id":"64672bef"},"outputs":[],"source":["def step( idx: int):\n","        \"\"\"\n","        ### Training Step\n","        \"\"\"\n","\n","        # Reset gradients\n","        discriminator_optimizer.zero_grad()\n","\n","        # Accumulate gradients for `gradient_accumulate_steps`\n","        for i in range(gradient_accumulate_steps):\n","\n","                    # Sample images from generator\n","                    generated_images, _ = generate_images(batch_size)\n","                    # Discriminator classification for generated images\n","                    fake_output = discriminator(generated_images.detach())\n","\n","                    # Get real images from the data loader\n","                    real_images = next(loader).to(device)\n","                    # We need to calculate gradients w.r.t. real images for gradient penalty\n","                    if (idx + 1) % lazy_gradient_penalty_interval == 0:\n","                        real_images.requires_grad_()\n","                    # Discriminator classification for real images\n","                    real_output = discriminator(real_images)\n","\n","                    # Get discriminator loss\n","                    real_loss, fake_loss = discriminator_loss(real_output, fake_output)\n","                    disc_loss = real_loss + fake_loss\n","\n","                    # Add gradient penalty\n","                    if (idx + 1) % lazy_gradient_penalty_interval == 0:\n","                        # Calculate and log gradient penalty\n","                        gp = gradient_penalty(real_images, real_output)\n","                        # Multiply by coefficient and add gradient penalty\n","                        disc_loss = disc_loss + 0.5 * gradient_penalty_coefficient * gp * lazy_gradient_penalty_interval\n","\n","                    # Compute gradients\n","                    disc_loss.backward()\n","\n","\n","\n","\n","\n","        # Clip gradients for stabilization\n","        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n","        # Take optimizer step\n","        discriminator_optimizer.step()\n","\n","        # Reset gradients\n","        generator_optimizer.zero_grad()\n","        mapping_network_optimizer.zero_grad()\n","\n","        # Accumulate gradients for `gradient_accumulate_steps`\n","        for i in range(gradient_accumulate_steps):\n","                # Sample images from generator\n","                generated_images, w = generate_images(batch_size)\n","                # Discriminator classification for generated images\n","                fake_output = discriminator(generated_images)\n","\n","                # Get generator loss\n","                gen_loss = generator_loss(fake_output)\n","\n","                # Add path length penalty\n","                if idx > lazy_path_penalty_after and (idx + 1) % lazy_path_penalty_interval == 0:\n","                    # Calculate path length penalty\n","                    plp = path_length_penalty(w, generated_images)\n","                    # Ignore if `nan`\n","                    if not torch.isnan(plp):\n","                        gen_loss = gen_loss + plp\n","\n","                # Calculate gradients\n","                gen_loss.backward()\n","\n","\n","\n","\n","\n","        # Clip gradients for stabilization\n","        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n","        torch.nn.utils.clip_grad_norm_(mapping_network.parameters(), max_norm=1.0)\n","\n","        # Take optimizer step\n","        generator_optimizer.step()\n","        mapping_network_optimizer.step()\n","\n","        utils.save_image(\n","                torch.cat([generated_images[:6], real_images[:3]], dim=0),\n","                os.path.join('checkpoints','sample.png'),\n","                nrow=3,\n","                normalize=True,\n","                value_range=(-1, 1),\n","            )\n","\n","\n","\n","        # Save model checkpoints\n","        if (idx + 1) % save_checkpoint_interval == 0:\n","            torch.save(generator.state_dict(), os.path.join('checkpoints','generator.pth'))\n","            torch.save(mapping_network.state_dict(), os.path.join('checkpoints','mapping_network.pth'))\n","            torch.save(discriminator.state_dict(), os.path.join('checkpoints','discriminator.pth'))"]},{"cell_type":"code","execution_count":null,"id":"bd3c766b","metadata":{"colab":{"referenced_widgets":["683a5683d89b4709a2f304d4ab28cabd"]},"id":"bd3c766b","outputId":"7a363923-de91-4389-d736-43fa7b403fd9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"683a5683d89b4709a2f304d4ab28cabd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/150000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Total number of training steps\n","training_steps: int = 150_000\n","# Loop for `training_steps`\n","for i in tqdm(range(training_steps)):\n","\n","            step(i)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2795c0dc","metadata":{"id":"2795c0dc"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
